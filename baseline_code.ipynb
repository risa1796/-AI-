{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itBUJtgSxnnD",
        "outputId": "dcdba42b-c4d3-4c5d-cb49-13750aafb028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/딥러닝기초/데이콘\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/딥러닝기초/데이콘"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore') "
      ],
      "metadata": {
        "id": "ELeezxLgyIMC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "mOeyioR-yMtZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Setting"
      ],
      "metadata": {
        "id": "jX3Y_O_cyQkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'IMG_SIZE':224,\n",
        "    'EPOCHS':5,\n",
        "    'LEARNING_RATE':3e-4,\n",
        "    'BATCH_SIZE':128,\n",
        "    'SEED':41\n",
        "}"
      ],
      "metadata": {
        "id": "FsDqv2WWyOvB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixed RandomSeed"
      ],
      "metadata": {
        "id": "7MilQeR0yT-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ],
      "metadata": {
        "id": "QygLAXqgyUVM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data load"
      ],
      "metadata": {
        "id": "xolLkO0TyaRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./train.csv')"
      ],
      "metadata": {
        "id": "hExs059GyV4O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train / validation split"
      ],
      "metadata": {
        "id": "Rn3wP7Aeyd_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1)\n",
        "train_len = int(len(df) * 0.8)"
      ],
      "metadata": {
        "id": "bUvhzghHyb6M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = df[:train_len]\n",
        "val = df[train_len:]"
      ],
      "metadata": {
        "id": "-m3v9k8yyh9R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing"
      ],
      "metadata": {
        "id": "p_6gSmTEylUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(df):\n",
        "    return df.iloc[:,2:].values"
      ],
      "metadata": {
        "id": "7gaBo0gbyjYb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = get_labels(train)\n",
        "val_labels = get_labels(val)"
      ],
      "metadata": {
        "id": "IepjY4cvym-O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataset"
      ],
      "metadata": {
        "id": "izQTSycIyqk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_path_list, label_list, transforms=None):\n",
        "        self.img_path_list = img_path_list\n",
        "        self.label_list = label_list\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.img_path_list[index]\n",
        "        \n",
        "        image = cv2.imread(img_path)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image=image)['image']\n",
        "        \n",
        "        if self.label_list is not None:\n",
        "            label = torch.FloatTensor(self.label_list[index])\n",
        "            return image, label\n",
        "        else:\n",
        "            return image\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_path_list)"
      ],
      "metadata": {
        "id": "NcuxBLnjyoRw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose([\n",
        "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
        "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "                            ToTensorV2()\n",
        "                            ])\n",
        "\n",
        "test_transform = A.Compose([\n",
        "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
        "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "                            ToTensorV2()\n",
        "                            ])"
      ],
      "metadata": {
        "id": "8zohwBlwyrbf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train['img_path'].values, train_labels, train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
        "\n",
        "val_dataset = CustomDataset(val['img_path'].values, val_labels, test_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "g5fEHBRTys3z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Define"
      ],
      "metadata": {
        "id": "hVfGywa1ywW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.backbone = models.efficientnet_b0(pretrained=True)\n",
        "        self.classifier = nn.Linear(1000, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = F.sigmoid(self.classifier(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "PiCAZQgmyuvY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "MrG3_atZyziN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "    model.to(device)\n",
        "    criterion = nn.BCELoss().to(device)\n",
        "    \n",
        "    best_val_acc = 0\n",
        "    best_model = None\n",
        "    \n",
        "    for epoch in range(1, CFG['EPOCHS']+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        for imgs, labels in tqdm(iter(train_loader)):\n",
        "            imgs = imgs.float().to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(imgs)\n",
        "            loss = criterion(output, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss.append(loss.item())\n",
        "                    \n",
        "        _val_loss, _val_acc = validation(model, criterion, val_loader, device)\n",
        "        _train_loss = np.mean(train_loss)\n",
        "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val ACC : [{_val_acc:.5f}]')\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step(_val_acc)\n",
        "            \n",
        "        if best_val_acc < _val_acc:\n",
        "            best_val_acc = _val_acc\n",
        "            best_model = model\n",
        "    \n",
        "    return best_model"
      ],
      "metadata": {
        "id": "7FLXFSa7yxXA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, criterion, val_loader, device):\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(iter(val_loader)):\n",
        "            imgs = imgs.float().to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            probs = model(imgs)\n",
        "            \n",
        "            loss = criterion(probs, labels)\n",
        "            \n",
        "            probs  = probs.cpu().detach().numpy()\n",
        "            labels = labels.cpu().detach().numpy()\n",
        "            preds = probs > 0.5\n",
        "            batch_acc = (labels == preds).mean()\n",
        "            \n",
        "            val_acc.append(batch_acc)\n",
        "            val_loss.append(loss.item())\n",
        "        \n",
        "        _val_loss = np.mean(val_loss)\n",
        "        _val_acc = np.mean(val_acc)\n",
        "    \n",
        "    return _val_loss, _val_acc"
      ],
      "metadata": {
        "id": "M6AvnNrcy0V8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "vW9UEGziy342"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BaseModel()\n",
        "model.eval()\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
        "\n",
        "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
      ],
      "metadata": {
        "id": "25I6NfpAy178"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "W5oa9Zx9y8au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('./test.csv')"
      ],
      "metadata": {
        "id": "4Q2eFrF4y6u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(test['img_path'].values, None, test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "KS4YJxf5y-30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for imgs in tqdm(iter(test_loader)):\n",
        "            imgs = imgs.float().to(device)\n",
        "            \n",
        "            probs = model(imgs)\n",
        "\n",
        "            probs  = probs.cpu().detach().numpy()\n",
        "            preds = probs > 0.5\n",
        "            preds = preds.astype(int)\n",
        "            predictions += preds.tolist()\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "TWKPAA_VzA-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = inference(model, test_loader, device)"
      ],
      "metadata": {
        "id": "ZHOECgQHzCeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission"
      ],
      "metadata": {
        "id": "IA0U75T6zDyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./sample_submission.csv')"
      ],
      "metadata": {
        "id": "DQeWmtukzEe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.iloc[:,1:] = preds\n",
        "submit.head()"
      ],
      "metadata": {
        "id": "Qrij3KgAzGe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.to_csv('./baseline_submit.csv', index=False)"
      ],
      "metadata": {
        "id": "XMno1nbFzH49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}